{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "588d210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "import time\n",
    "\n",
    "import torch.utils.data as data_utils\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch import nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy.io import savemat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c2b8bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "\n",
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "def _transform(n_px):\n",
    "    return Compose([\n",
    "        Resize(n_px, interpolation=BICUBIC),\n",
    "        CenterCrop(n_px),\n",
    "        _convert_image_to_rgb,\n",
    "        ToTensor(),\n",
    "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "    ])\n",
    "nx=144\n",
    "\n",
    "import clip\n",
    "print(clip.available_models())\n",
    "model, preprocess = clip.load('RN50x4', device)\n",
    "\n",
    "relu = torch.nn.functional.relu\n",
    "def features(net, x):\n",
    "    x = x.type(net.conv1.weight.dtype)\n",
    "    for conv, bn in [(net.conv1, net.bn1), (net.conv2, net.bn2), (net.conv3, net.bn3)]:\n",
    "        x = relu(bn(conv(x)))\n",
    "    x = net.avgpool(x)\n",
    "    x = net.layer1(x)\n",
    "    x = net.layer2(x)\n",
    "    x = net.layer3(x)\n",
    "    x = net.layer4(x)\n",
    "    x=net.avgpool(x) \n",
    "    x=net.avgpool(x)\n",
    "    return x\n",
    "\n",
    "# Download the dataset\n",
    "cifar100_train = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=True, transform=_transform(nx))\n",
    "cifar100_test = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False, transform=_transform(nx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94681d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def generate_features(dataset,model):\n",
    "    labels = torch.empty(0).cpu()\n",
    "    i = 0\n",
    "    for images, labs in tqdm(DataLoader(dataset, batch_size=100)):\n",
    "            images = images.to(device)\n",
    "            labs = labs.cpu()\n",
    "            with torch.no_grad():\n",
    "                f = features(model.visual,images) \n",
    "                f = f.squeeze((2,3))\n",
    "            f=f.cpu()\n",
    "            if i==0:\n",
    "                d=f.shape[1]\n",
    "                feat = torch.empty(0, d).cpu()\n",
    "            feat = torch.cat((feat,f),dim=0)\n",
    "            labels = torch.cat((labels , labs),dim=0)\n",
    "            i = i+1\n",
    "    return feat, labels\n",
    "\n",
    "features_train,labels_train = generate_features(cifar100_train,model)\n",
    "features_test,labels_test = generate_features(cifar100_test,model)\n",
    "\n",
    "for i in range(100):\n",
    "    print(i)\n",
    "    name = r'D:\\Cifar100\\features\\Clip\\val'+'{}'.format(i) +'.mat'\n",
    "    x = features_test[labels_test == i,:]\n",
    "    savemat(name,{'feature':x.float().numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8b3613e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = 100 #number of classes\n",
    "d = 2560 #number of features\n",
    "#Loading data\n",
    "def load_data(path,file):\n",
    "    name=path+file\n",
    "    m=loadmat(name)\n",
    "    x=torch.tensor(m['feature'])\n",
    "    return x.float()\n",
    "\n",
    "path = r'D:\\Cifar100\\features\\Clip\\train/' \n",
    "trFeatures = torch.empty((0,d))\n",
    "trY  = torch.empty(0)\n",
    "for j in range(0,nc):\n",
    "    x = load_data(path,'train{}.mat'.format(j))\n",
    "    rep = x.shape[0]\n",
    "    y =  torch.tensor(j)\n",
    "    y1 = y.repeat(rep)\n",
    "    trFeatures = torch.cat((trFeatures,x),dim = 0)\n",
    "    trY = torch.cat((trY,y1),dim=0)\n",
    "\n",
    "path = r'D:\\Cifar100\\features\\Clip\\val/'\n",
    "valFeatures = torch.empty((0,d))\n",
    "valY  = torch.empty(0)\n",
    "for j in range(0,nc):\n",
    "    x = load_data(path,'val{}.mat'.format(j))\n",
    "    rep = x.shape[0]\n",
    "    y =  torch.tensor(j)\n",
    "    y1 = y.repeat(rep)\n",
    "    valFeatures = torch.cat((valFeatures,x), dim = 0)\n",
    "    valY = torch.cat((valY,y1),dim=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2819c092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d883c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fs_ppca(trFeatures, trY, nc, d, q=20):\n",
    "    \"\"\"\n",
    "    Performs Feature Selection using Probabilistic Principal Component Analysis (PPCA)\n",
    "    for each class and returns a list of feature indices sorted by their relevance.\n",
    "\n",
    "    Args:\n",
    "        trFeatures (torch.Tensor): Training features (N x d), where N is the number of samples\n",
    "                                     and d is the number of features.\n",
    "        trY (torch.Tensor): Training labels (N).\n",
    "        nc (int): Number of classes.\n",
    "        d (int): Number of features.\n",
    "        q (int, optional): Number of principal components to retain. Defaults to 20.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (nc x d) containing feature indices sorted in\n",
    "                      descending order of their relevance for each class.\n",
    "    \"\"\"\n",
    "    index_list = torch.empty((0, d), dtype=torch.int32)\n",
    "    for i in range(nc):\n",
    "        data = trFeatures[trY == i, :]\n",
    "        mu = torch.mean(data, dim=0)\n",
    "        data_centered = (data - mu).t()  # Center the data\n",
    "\n",
    "        # Perform Singular Value Decomposition (SVD)\n",
    "        U, S, V = torch.linalg.svd(data_centered)\n",
    "        S_squared = S**2\n",
    "\n",
    "        # Estimate the noise variance (sig_ML)\n",
    "        sig_ML = (1 / (d - q)) * torch.sum(S_squared[q:])\n",
    "\n",
    "        # Construct the loading matrix A_ML\n",
    "        eigenvalues_adjusted = torch.diag(S_squared[:q]) - sig_ML * torch.eye(q)\n",
    "        # Ensure the diagonal elements are non-negative before taking the square root\n",
    "#        eigenvalues_adjusted = torch.relu(eigenvalues_adjusted)\n",
    "        A_ML = U[:, :q] @ (eigenvalues_adjusted**(0.5))\n",
    "\n",
    "        # Calculate the row sums of squares (feature relevance)\n",
    "        row_ss = torch.sum(A_ML**2, dim=1)\n",
    "\n",
    "        # Get the indices sorted by row sums of squares in descending order\n",
    "        indices = torch.argsort(-row_ss)\n",
    "        index_list = torch.cat((index_list, indices.unsqueeze(0)), dim=0)\n",
    "\n",
    "    return index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "df25f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EMv2(X, S_x, W, Psi, r, n, max_iter=30):\n",
    "    \"\"\"\n",
    "    Performs Expectation-Maximization (EM) algorithm for PPCA.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): Data matrix (d x n).\n",
    "        S_x (torch.Tensor): Sample covariance matrix (d x d).\n",
    "        W (torch.Tensor): Initial loading matrix (d x r).\n",
    "        Psi (torch.Tensor): Initial noise variance (d).\n",
    "        r (int): Number of principal components.\n",
    "        n (int): Number of samples.\n",
    "        max_iter (int, optional): Maximum number of EM iterations. Defaults to 20.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Updated loading matrix (W) and noise variance (Psi).\n",
    "    \"\"\"\n",
    "    for t in range(max_iter):\n",
    "        Psi_d = torch.diag(Psi)\n",
    "        M1 = torch.linalg.inv(Psi_d + W @ W.t())\n",
    "        beta = W.t() @ M1\n",
    "        E_gam = beta @ X\n",
    "        M2 = E_gam @ E_gam.t()\n",
    "        E_ggam = n * torch.eye(r) - n * beta @ W\n",
    "        E_ggam = E_ggam + M2\n",
    "\n",
    "        M3 = torch.linalg.inv(E_ggam)\n",
    "        W_new = X @ E_gam.t()\n",
    "        W_new = W_new @ M3\n",
    "        M4 = beta @ S_x\n",
    "        Psi_new = torch.diag(S_x - W_new @ M4)\n",
    "        Psi_new = torch.clamp(Psi_new, min=1e-6) # Ensure Psi is positive\n",
    "\n",
    "        W = W_new\n",
    "        Psi = Psi_new\n",
    "    return (W, Psi)\n",
    "\n",
    "def LFA_st(u, s, q, d):\n",
    "    \"\"\"\n",
    "    Initializes the parameters for Linear Factor Analysis (LFA).\n",
    "\n",
    "    Args:\n",
    "        u (torch.Tensor): Left singular vectors from SVD.\n",
    "        s (torch.Tensor): Singular values from SVD.\n",
    "        q (int): Number of principal components.\n",
    "        d (int): Number of features.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Initial loading matrix (A_ML) and noise variance (Psi).\n",
    "    \"\"\"\n",
    "    sig_ML = (1 / (d - q)) * torch.sum(s[q:])\n",
    "    A_ML = u[:, :q] @ ((torch.diag(s[:q]) - sig_ML * torch.eye(q))**(0.5))\n",
    "    Psi = sig_ML * torch.ones(d)\n",
    "    return (A_ML, Psi)\n",
    "\n",
    "def fs_lfa(trFeatures, trY, nc, d, q=20):\n",
    "    \"\"\"\n",
    "    Performs Feature Selection using Linear Factor Analysis (LFA) for each class.\n",
    "\n",
    "    Args:\n",
    "        trFeatures (torch.Tensor): Training features (N x d).\n",
    "        trY (torch.Tensor): Training labels (N).\n",
    "        nc (int): Number of classes.\n",
    "        d (int): Number of features.\n",
    "        q (int, optional): Number of principal components to retain. Defaults to 20.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (nc x d) containing feature indices sorted in\n",
    "                      descending order of their relevance (SNR) for each class.\n",
    "    \"\"\"\n",
    "    index_list = torch.empty((0, d), dtype=torch.int32)\n",
    "    for i in range(nc):\n",
    "        data = trFeatures[trY == i, :]\n",
    "        n = data.shape[0]\n",
    "        mu = torch.mean(data, dim=0)\n",
    "        data_centered_t = (data - mu).t()\n",
    "        S = torch.cov(data_centered_t) + 0.01 * torch.eye(d)\n",
    "        u, s, _ = torch.linalg.svd(data_centered_t)\n",
    "        W_st, Psi_st = LFA_st(u, s**2, q, d)\n",
    "        W_ML, Psi_ML = EMv2(data_centered_t, S, W_st, Psi_st, q, n)\n",
    "        snr = torch.sum(W_ML**2, dim=1) / Psi_ML\n",
    "        indices = torch.argsort(-snr)\n",
    "        index_list = torch.cat((index_list, indices.unsqueeze(0)), dim=0)\n",
    "    return index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "34861e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELF\n",
    "def ELF_st(X, r, d):\n",
    "    \"\"\"\n",
    "    Initializes parameters for Exploratory Latent Factor (ELF) model.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): Data matrix (n x d).\n",
    "        r (int): Number of latent factors.\n",
    "        d (int): Number of features.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Initial latent factors (Gamma) and noise variance (Psi).\n",
    "    \"\"\"\n",
    "    u, s, v = torch.linalg.svd(X)\n",
    "    Gamma = u[:, :r] * s[:r]\n",
    "    Psi = torch.ones(d, dtype=torch.float32)\n",
    "    return (Gamma, Psi)\n",
    "\n",
    "def ELF(X0, Gamma, Psi, n, epochs, r, d, tolerance=0.1):\n",
    "    \"\"\"\n",
    "    Performs Exploratory Latent Factor (ELF) algorithm.\n",
    "\n",
    "    Args:\n",
    "        X0 (torch.Tensor): Centered data matrix (n x d).\n",
    "        Gamma (torch.Tensor): Initial latent factors (n x r).\n",
    "        Psi (torch.Tensor): Initial noise variance (d).\n",
    "        n (int): Number of samples.\n",
    "        epochs (int): Maximum number of iterations.\n",
    "        r (int): Number of latent factors.\n",
    "        d (int): Number of features.\n",
    "        tolerance (float, optional): Convergence tolerance. Defaults to 0.1.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Signal-to-noise ratio (SNR) for each feature (d).\n",
    "    \"\"\"\n",
    "    W = torch.ones((d, r))\n",
    "    for ep in range(epochs):\n",
    "        # print(ep)\n",
    "        # Updating W\n",
    "        W_new = X0.t() @ Gamma\n",
    "        M1 = torch.linalg.inv(Gamma.t() @ Gamma)\n",
    "        W_new = W_new @ M1\n",
    "        # Updating Gamma\n",
    "        m1 = torch.diag(1 / Psi) @ W_new\n",
    "        invMat = torch.linalg.inv(W_new.t() @ m1)\n",
    "        Gamma = X0 @ m1 @ invMat\n",
    "        # Orthogonalisation\n",
    "        u, s, v = torch.linalg.svd(Gamma)\n",
    "        M = v.t() @ torch.diag(s)\n",
    "        W_new = (W_new @ M) / np.sqrt(n)\n",
    "        Gamma = u[:, :r] * np.sqrt(n)\n",
    "        # Updating weights (Psi)\n",
    "        Xhat = Gamma @ W_new.t()\n",
    "        Psi = torch.mean((X0 - Xhat)**2, dim=0)\n",
    "        Psi[Psi < 0.01] = 0.01\n",
    "\n",
    "        if torch.sqrt(torch.sum((W - W_new)**2)) < tolerance:\n",
    "            print('ep', ep)\n",
    "            break\n",
    "        W = W_new\n",
    "    snr = torch.sum(W_new**2, dim=1) / Psi\n",
    "    return snr\n",
    "\n",
    "epochs_elf = 10\n",
    "r_elf = 10\n",
    "def fs_ELF(trFeatures, trY, nc, d, r=r_elf, epochs=epochs_elf):\n",
    "    \"\"\"\n",
    "    Performs Self-Exploratory Latent Factor (SELF) based feature selection for each class.\n",
    "\n",
    "    Args:\n",
    "        trFeatures (torch.Tensor): Training features (N x d).\n",
    "        trY (torch.Tensor): Training labels (N).\n",
    "        nc (int): Number of classes.\n",
    "        d (int): Number of features.\n",
    "        r (int, optional): Number of latent factors. Defaults to r_elf (10).\n",
    "        epochs (int, optional): Maximum number of ELF iterations. Defaults to epochs_elf (100).\n",
    "        device (str, optional): Device to run computations on ('cpu' or 'cuda'). Defaults to 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (nc x d) containing feature indices sorted in\n",
    "                      descending order of their relevance (SNR) for each class.\n",
    "    \"\"\"\n",
    "    idx_list = torch.empty((0, d), dtype=torch.int32)\n",
    "\n",
    "    for i in range(nc):\n",
    "        X = trFeatures[trY == i, :]\n",
    "        mu = torch.mean(X, dim=0)\n",
    "        X0 = (X - mu)\n",
    "\n",
    "        Gamma, Psi = ELF_st(X0, r, d)\n",
    "        n = X0.shape[0]\n",
    "        snr = ELF(X0, Gamma, Psi, n, epochs, r, d)\n",
    "        idx = torch.argsort(-snr).to(device)\n",
    "        idx_list = torch.cat((idx_list, idx.unsqueeze(0)), dim=0)\n",
    "    return idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6b758bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPCA\n",
    "T_hpca = 5\n",
    "r_hpca = 10\n",
    "def heteroPCA(Cov, r, T, d):\n",
    "    \"\"\"\n",
    "    Performs Heteroscedastic Principal Component Analysis (HPCA).\n",
    "\n",
    "    Args:\n",
    "        Cov (torch.Tensor): Covariance matrix (d x d).\n",
    "        r (int): Number of principal components.\n",
    "        T (int): Number of iterations.\n",
    "        d (int): Number of features.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Principal components (u) and the modified covariance matrix (N_tilda).\n",
    "    \"\"\"\n",
    "    d_indi = torch.arange(d)\n",
    "    N_prev = Cov - torch.diag(torch.diagonal(Cov))\n",
    "    for t in range(T):\n",
    "        # print(t)\n",
    "        u, s, v = torch.linalg.svd(N_prev)\n",
    "        N_tilda = u[:, :r] @ torch.diag(s[:r]) @ v[:r, :]\n",
    "        N_tilda_D = torch.diagonal(N_tilda)\n",
    "        N_prev[d_indi, d_indi] = N_tilda_D\n",
    "    return (u[:, :r], N_tilda)\n",
    "\n",
    "def fs_HPCA(trFeatures, trY, nc, d, r=r_hpca, T=T_hpca):\n",
    "    \"\"\"\n",
    "    Performs Feature Selection using Heteroscedastic Probabilistic Principal Component Analysis (HPCA)\n",
    "    for each class.\n",
    "\n",
    "    Args:\n",
    "        trFeatures (torch.Tensor): Training features (N x d).\n",
    "        trY (torch.Tensor): Training labels (N).\n",
    "        nc (int): Number of classes.\n",
    "        d (int): Number of features.\n",
    "        r (int, optional): Number of principal components for HPCA. Defaults to r_hpca (10).\n",
    "        T (int, optional): Number of iterations for HPCA. Defaults to T_hpca (5).\n",
    "        device (str, optional): Device to run computations on ('cpu' or 'cuda'). Defaults to 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (nc x d) containing feature indices sorted in\n",
    "                      descending order of their relevance (SNR-like metric) for each class.\n",
    "    \"\"\"\n",
    "    idx_list = torch.empty((0, d), dtype=torch.int32)\n",
    "    for i in range(nc):\n",
    "        X = trFeatures[trY == i, :]\n",
    "        mu = torch.mean(X, dim=0)\n",
    "        # std = torch.std(X,dim=0) # Standard deviation is not used here\n",
    "        X_centered_t = (X - mu).t()\n",
    "        Cov = torch.cov(X_centered_t) + 0.01 * torch.eye(d)\n",
    "        u, _ = heteroPCA(Cov, r, T, d)\n",
    "\n",
    "        Gamma = X_centered_t.t() @ u\n",
    "        W = u\n",
    "\n",
    "        n = X_centered_t.shape[1]\n",
    "        u_gamma, s_gamma, v_gamma = torch.linalg.svd(Gamma)\n",
    "        M = v_gamma.t() @ torch.diag(s_gamma)\n",
    "        W_new = (W @ M) / np.sqrt(n)\n",
    "        Gamma_new = u_gamma[:, :r] * np.sqrt(n)\n",
    "\n",
    "        X_hat = Gamma_new @ W_new.t()\n",
    "\n",
    "        Psi = torch.mean((X_centered_t.t() - X_hat)**2, dim=0)\n",
    "        Sig = torch.sum(W_new**2, dim=1)\n",
    "\n",
    "        snr_e = Sig / Psi\n",
    "        idx = torch.argsort(-snr_e).to(device)\n",
    "        idx_list = torch.cat((idx_list, idx.unsqueeze(0)), dim=0)\n",
    "    return idx_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fa6c5127",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MahalanobisClassifier:\n",
    "    \"\"\"\n",
    "    A classifier that uses Mahalanobis distance for classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, device='cpu'):\n",
    "        \"\"\"\n",
    "        Initializes the MahalanobisClassifier.\n",
    "\n",
    "        Args:\n",
    "            device (str, optional): The device to perform computations on ('cpu' or 'cuda'). Defaults to 'cpu'.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.means = []\n",
    "        self.inv_covariances = []\n",
    "\n",
    "    def _calculate_class_parameters(self, index_list, features_train, trY, nc, nf):\n",
    "        \"\"\"\n",
    "        Calculates the mean and inverse covariance matrix for the selected features of each class.\n",
    "\n",
    "        Args:\n",
    "            index_list (torch.Tensor): Tensor of feature indices for each class (nc x nf).\n",
    "            features_train (torch.Tensor): Training features (N x d).\n",
    "            trY (torch.Tensor): Training labels (N).\n",
    "            nc (int): Number of classes.\n",
    "            nf (int): Number of selected features.\n",
    "        \"\"\"\n",
    "        self.means = []\n",
    "        self.inv_covariances = []\n",
    "        for i in range(nc):\n",
    "            indices = index_list[i]\n",
    "            features = features_train[trY == i, :]\n",
    "            features_selected = features[:, indices]\n",
    "\n",
    "            mean = torch.mean(features_selected, dim=0).to(self.device)\n",
    "            self.means.append(mean)\n",
    "\n",
    "            cov = torch.cov(features_selected.t()) + 0.1 * torch.eye(nf, device=self.device)\n",
    "            inv_cov = torch.linalg.inv(cov)\n",
    "            self.inv_covariances.append(inv_cov)\n",
    "\n",
    "    def _mahalanobis_distance(self, x, inv_cov):\n",
    "        \"\"\"\n",
    "        Calculates the Mahalanobis distance.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Data point (1 x nf).\n",
    "            inv_cov (torch.Tensor): Inverse covariance matrix (nf x nf).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Mahalanobis distance.\n",
    "        \"\"\"\n",
    "        diff = (x - self.means[self.current_class]).to(self.device)\n",
    "        dist = (diff @ inv_cov) @ diff.t()\n",
    "        return dist\n",
    "\n",
    "    def classify(self, features_valid, index_list, nc, n_val):\n",
    "        \"\"\"\n",
    "        Classifies validation features based on Mahalanobis distance to class means.\n",
    "\n",
    "        Args:\n",
    "            features_valid (torch.Tensor): Validation features (N_val x d).\n",
    "            index_list (torch.Tensor): Tensor of feature indices for each class (nc x nf).\n",
    "            nc (int): Number of classes.\n",
    "            n_val (int): Number of validation samples.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Predicted class labels for the validation set.\n",
    "        \"\"\"\n",
    "        out = torch.zeros(n_val, nc, device=self.device)\n",
    "        for i in range(nc):\n",
    "            self.current_class = i\n",
    "            indices = index_list[i]\n",
    "            x_valid_selected = features_valid[:, indices].to(self.device)\n",
    "            inv_cov = self.inv_covariances[i]\n",
    "            for j in range(n_val):\n",
    "                out[j, i] = self._mahalanobis_distance(x_valid_selected[j], inv_cov)\n",
    "        predicted_labels = torch.argmin(out, dim=1).cpu()\n",
    "        return predicted_labels\n",
    "\n",
    "def calculate_accuracy(classifier, features_train, features_valid, index_list, valY, trY, nc, nf):\n",
    "    \"\"\"\n",
    "    Calculates the classification accuracy using the MahalanobisClassifier.\n",
    "\n",
    "    Args:\n",
    "        classifier (MahalanobisClassifier): An instance of the MahalanobisClassifier.\n",
    "        features_train (torch.Tensor): Training features (N x d).\n",
    "        features_valid (torch.Tensor): Validation features (N_val x d).\n",
    "        index_list (torch.Tensor): Tensor of feature indices for each class (nc x nf).\n",
    "        valY (torch.Tensor): Validation labels (N_val).\n",
    "        trY (torch.Tensor): Training labels (N).\n",
    "        nc (int): Number of classes.\n",
    "        nf (int): Number of selected features.\n",
    "\n",
    "    Returns:\n",
    "        float: The classification accuracy.\n",
    "    \"\"\"\n",
    "    n_val = features_valid.shape[0]\n",
    "    classifier._calculate_class_parameters(index_list, features_train, trY, nc, nf)\n",
    "    predictions = classifier.classify(features_valid, index_list, nc, n_val)\n",
    "    accuracy = np.round((torch.sum(predictions == valY.cpu()).item() / n_val), 4)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8a69c83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1bfb9966",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dic={}\n",
    "time_dic={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c931775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection using PPCA\n",
    "start = time.time()\n",
    "index_lst = fs_ppca(trFeatures, trY, nc, d, q=20)\n",
    "index_dic['PPCA'] = index_lst\n",
    "end = time.time()\n",
    "time_dic['PPCA'] = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6da9804d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PPCA': 8.657537698745728}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24bc1c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection using LFA\n",
    "index_lst = fs_lfa(trFeatures, trY, nc, d, q=20)\n",
    "index_dic['LFA'] = index_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d31f6962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection using ELF\n",
    "index_lst = fs_ELF(trFeatures, trY, nc, d, r=r_elf, epochs=epochs_elf)\n",
    "index_dic['ELF'] = index_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ca3aa9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection using HPCA\n",
    "index_lst = fs_HPCA(trFeatures, trY, nc, d, r=r_hpca, T=T_hpca)\n",
    "index_dic['HPCA'] = index_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c343975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = torch.mean(trFeatures,dim = 0)\n",
    "std = torch.std(trFeatures,dim=0)\n",
    "\n",
    "trfeatures = (trFeatures-mu)/std\n",
    "valfeatures = (valFeatures -mu)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "de66617a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating PPCA with 750 features...\n",
      "PPCA (750 features) Accuracy: 0.6547\n",
      "Evaluating PPCA with 1000 features...\n",
      "PPCA (1000 features) Accuracy: 0.6936\n",
      "Evaluating PPCA with 1250 features...\n",
      "PPCA (1250 features) Accuracy: 0.7083\n",
      "Evaluating PPCA with 1500 features...\n",
      "PPCA (1500 features) Accuracy: 0.7212\n",
      "Evaluating PPCA with 1750 features...\n",
      "PPCA (1750 features) Accuracy: 0.7254\n",
      "Evaluating PPCA with 2000 features...\n",
      "PPCA (2000 features) Accuracy: 0.7301\n",
      "Evaluating PPCA with 2250 features...\n",
      "PPCA (2250 features) Accuracy: 0.7283\n",
      "Evaluating PPCA with 2560 features...\n",
      "PPCA (2560 features) Accuracy: 0.7281\n",
      "Evaluating LFA with 750 features...\n",
      "LFA (750 features) Accuracy: 0.6478\n",
      "Evaluating LFA with 1000 features...\n",
      "LFA (1000 features) Accuracy: 0.6736\n",
      "Evaluating LFA with 1250 features...\n",
      "LFA (1250 features) Accuracy: 0.6933\n",
      "Evaluating LFA with 1500 features...\n",
      "LFA (1500 features) Accuracy: 0.7096\n",
      "Evaluating LFA with 1750 features...\n",
      "LFA (1750 features) Accuracy: 0.7168\n",
      "Evaluating LFA with 2000 features...\n",
      "LFA (2000 features) Accuracy: 0.7227\n",
      "Evaluating LFA with 2250 features...\n",
      "LFA (2250 features) Accuracy: 0.7257\n",
      "Evaluating LFA with 2560 features...\n",
      "LFA (2560 features) Accuracy: 0.7281\n",
      "Evaluating ELF with 750 features...\n",
      "ELF (750 features) Accuracy: 0.6443\n",
      "Evaluating ELF with 1000 features...\n",
      "ELF (1000 features) Accuracy: 0.6738\n",
      "Evaluating ELF with 1250 features...\n",
      "ELF (1250 features) Accuracy: 0.6932\n",
      "Evaluating ELF with 1500 features...\n",
      "ELF (1500 features) Accuracy: 0.707\n",
      "Evaluating ELF with 1750 features...\n",
      "ELF (1750 features) Accuracy: 0.717\n",
      "Evaluating ELF with 2000 features...\n",
      "ELF (2000 features) Accuracy: 0.7225\n",
      "Evaluating ELF with 2250 features...\n",
      "ELF (2250 features) Accuracy: 0.7255\n",
      "Evaluating ELF with 2560 features...\n",
      "ELF (2560 features) Accuracy: 0.7281\n",
      "Evaluating HPCA with 750 features...\n",
      "HPCA (750 features) Accuracy: 0.6451\n",
      "Evaluating HPCA with 1000 features...\n",
      "HPCA (1000 features) Accuracy: 0.6706\n",
      "Evaluating HPCA with 1250 features...\n",
      "HPCA (1250 features) Accuracy: 0.6887\n",
      "Evaluating HPCA with 1500 features...\n",
      "HPCA (1500 features) Accuracy: 0.7028\n",
      "Evaluating HPCA with 1750 features...\n",
      "HPCA (1750 features) Accuracy: 0.7136\n",
      "Evaluating HPCA with 2000 features...\n",
      "HPCA (2000 features) Accuracy: 0.7151\n",
      "Evaluating HPCA with 2250 features...\n",
      "HPCA (2250 features) Accuracy: 0.7201\n",
      "Evaluating HPCA with 2560 features...\n",
      "HPCA (2560 features) Accuracy: 0.7281\n",
      "\n",
      "Classification Accuracy for Different Number of Features:\n",
      "PPCA:\n",
      "  750 features: 0.6547\n",
      "  1000 features: 0.6936\n",
      "  1250 features: 0.7083\n",
      "  1500 features: 0.7212\n",
      "  1750 features: 0.7254\n",
      "  2000 features: 0.7301\n",
      "  2250 features: 0.7283\n",
      "  2560 features: 0.7281\n",
      "LFA:\n",
      "  750 features: 0.6478\n",
      "  1000 features: 0.6736\n",
      "  1250 features: 0.6933\n",
      "  1500 features: 0.7096\n",
      "  1750 features: 0.7168\n",
      "  2000 features: 0.7227\n",
      "  2250 features: 0.7257\n",
      "  2560 features: 0.7281\n",
      "ELF:\n",
      "  750 features: 0.6443\n",
      "  1000 features: 0.6738\n",
      "  1250 features: 0.6932\n",
      "  1500 features: 0.707\n",
      "  1750 features: 0.717\n",
      "  2000 features: 0.7225\n",
      "  2250 features: 0.7255\n",
      "  2560 features: 0.7281\n",
      "HPCA:\n",
      "  750 features: 0.6451\n",
      "  1000 features: 0.6706\n",
      "  1250 features: 0.6887\n",
      "  1500 features: 0.7028\n",
      "  1750 features: 0.7136\n",
      "  2000 features: 0.7151\n",
      "  2250 features: 0.7201\n",
      "  2560 features: 0.7281\n"
     ]
    }
   ],
   "source": [
    "nf_list = [750, 1000, 1250, 1500, 1750, 2000, 2250, 2560]\n",
    "accuracy_results = {}\n",
    "classifier = MahalanobisClassifier()\n",
    "for method, indices in index_dic.items():\n",
    "    accuracy_list = []\n",
    "    for nf in nf_list:\n",
    "        print(f\"Evaluating {method} with {nf} features...\")\n",
    "        index_sel = indices[:, :nf]\n",
    "        accuracy = calculate_accuracy(classifier, trfeatures, valfeatures, index_sel, valY, trY, nc, nf)\n",
    "        accuracy_list.append(accuracy)\n",
    "        print(f\"{method} ({nf} features) Accuracy: {accuracy}\")\n",
    "    accuracy_results[method] = accuracy_list\n",
    "\n",
    "\n",
    "print(\"\\nClassification Accuracy for Different Number of Features:\")\n",
    "for method, accuracies in accuracy_results.items():\n",
    "    print(f\"{method}:\")\n",
    "    for i, nf in enumerate(nf_list):\n",
    "        print(f\"  {nf} features: {accuracies[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a0ab6392",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_utils.TensorDataset(trfeatures, trY)\n",
    "test_data = data_utils.TensorDataset(valfeatures, valY)\n",
    "######################################################################################################################\n",
    "\n",
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.zero_()\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        return out\n",
    "\n",
    "######################################################################################################################\n",
    "def CorrectPred(outputs, responses):\n",
    "    softmax = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    prob = softmax.detach().cpu().numpy()\n",
    "    predictions = np.argmax(prob, axis=1)\n",
    "    res = responses.long().cpu().numpy()\n",
    "    return np.sum(predictions == res)\n",
    "\n",
    "def complement(total_dim, indices):\n",
    "    all_indices = torch.arange(0, total_dim)\n",
    "    mask = torch.ones(total_dim, dtype=torch.bool)\n",
    "    mask[indices] = False\n",
    "    complement_indices = all_indices[mask]\n",
    "    return complement_indices\n",
    "######################################################################################################################\n",
    "def FSA(input_dim, w1,k,mu_fsa,current_epoch,M_s,epochs):\n",
    "    M = int(np.floor(k + (input_dim - k) * max(0, (epochs - 2 * current_epoch) / (2 * current_epoch * mu_fsa + epochs))))\n",
    "    M_s[current_epoch] = M\n",
    "    if (current_epoch > 0) and (M_s[current_epoch] <= M_s[current_epoch - 1]):\n",
    "        crit = torch.sum(w1**2, dim=0)\n",
    "        ordered_ind = torch.argsort(-crit)[M:]\n",
    "        w1[:, ordered_ind] = 0\n",
    "    return(w1,M_s)\n",
    "\n",
    "def TISP(w1,k):\n",
    "    crit = torch.sum(w1**2,dim=0)\n",
    "    orderd_ind = torch.argsort(-crit)\n",
    "    selected_ind = orderd_ind[0:k]\n",
    "    lamb = crit[orderd_ind[k]]**0.5\n",
    "    mask = complement(d,selected_ind)\n",
    "    w1 = w1 - lamb\n",
    "    w1[:,mask] = torch.zeros((nc,d-k))\n",
    "    return(w1)\n",
    "    \n",
    "def train_and_evaluate(input_dim, output_dim, method, k, epochs=30, learning_rate=0.01, mu_fsa=20, device='cpu'):\n",
    "    model = FullyConnected(input_dim, output_dim).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = CrossEntropyLoss().to(device)\n",
    "\n",
    "    acc_history_train = []\n",
    "    acc_history_test = []\n",
    "    elapsed_time_total = 0\n",
    "    M_s = np.zeros(epochs)\n",
    "    \n",
    "    batchsize = 128  # Initial batch size\n",
    "    train_loader = data_utils.DataLoader(train_data, batch_size=batchsize, shuffle=True, num_workers=0)\n",
    "    test_loader = data_utils.DataLoader(test_data, batch_size=batchsize, shuffle=True, num_workers=0)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        for i, (data, Y) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            Y = Y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = loss_fn(outputs, Y.long())\n",
    "            correct_train += CorrectPred(outputs, Y)\n",
    "            total_train += Y.size(0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        acc_train = correct_train / total_train\n",
    "        acc_history_train.append(acc_train)\n",
    "\n",
    "        w1 = model.fc1.weight.detach()\n",
    "        if method == 'FSA':\n",
    "            w1,M_s = FSA(input_dim,w1,k,mu_fsa,epoch,M_s,epochs)\n",
    "        if method == 'TISP':\n",
    "            w1 = TISP(w1,k)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            model.fc1.weight = nn.Parameter(w1)\n",
    "        elapsed_time_epoch = time.time() - start_time\n",
    "        elapsed_time_total += elapsed_time_epoch\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            correct_test = 0\n",
    "            total_test = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for data_test, Y_test in test_loader:\n",
    "                    data_test = data_test.to(device)\n",
    "                    Y_test = Y_test.to(device)\n",
    "                    outputs_test = model(data_test)\n",
    "                    correct_test += CorrectPred(outputs_test, Y_test)\n",
    "                    total_test += Y_test.size(0)\n",
    "            acc_test = correct_test / total_test\n",
    "            acc_history_test.append(acc_test)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            batchsize *= 2\n",
    "            train_loader = data_utils.DataLoader(train_data, batch_size=batchsize, shuffle=True, num_workers=0)\n",
    "            test_loader = data_utils.DataLoader(test_data, batch_size=batchsize, shuffle=True, num_workers=0)\n",
    "            print(f\"  Batch size increased to: {batchsize}\")\n",
    "\n",
    "    print(f\"Total Training Time: {elapsed_time_total:.2f}s\")\n",
    "    print(f\"  Test Accuracy: {acc_test:.4f}\")\n",
    "    return acc_history_test,elapsed_time_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11af3453",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['FSA','TISP']\n",
    "def get_accuracy_for_nf(train_loader,test_loader, d, nc, nf_list):\n",
    "    accuracy_results = {}\n",
    "    time = {}\n",
    "    for m in methods:\n",
    "        for nf in nf_list:\n",
    "            print(f\"\\nTraining and evaluating with {nf} features...\")\n",
    "            acc_test_hist, training_time = train_and_evaluate(d, nc, m, nf, epochs=30, learning_rate=0.01, mu_fsa=20, device='cpu')\n",
    "            accuracy_results[nf] = acc_test_hist[-1]\n",
    "            if m not in time.keys():\n",
    "                time[m] = [training_time]\n",
    "                accuracy_results[m] = [acc_test_hist[-1]]\n",
    "            else:\n",
    "                time[m].append(training_time)\n",
    "                accuracy_results[m].append(acc_test_hist[-1])\n",
    "    return accuracy_results,training_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24af53f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating with 750 features...\n",
      "Epoch 1/30\n",
      "Epoch 2/30\n",
      "Epoch 3/30\n",
      "Epoch 4/30\n",
      "Epoch 5/30\n",
      "Epoch 6/30\n",
      "Epoch 7/30\n",
      "Epoch 8/30\n",
      "Epoch 9/30\n",
      "Epoch 10/30\n",
      "  Batch size increased to: 256\n",
      "Epoch 11/30\n",
      "Epoch 12/30\n",
      "Epoch 13/30\n",
      "Epoch 14/30\n",
      "Epoch 15/30\n",
      "Epoch 16/30\n",
      "Epoch 17/30\n"
     ]
    }
   ],
   "source": [
    "accuracy, training_time = get_accuracy_for_nf(train_loader,test_loader, d, nc, nf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35238d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
